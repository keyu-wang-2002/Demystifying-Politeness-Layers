# ğŸ“˜ Politeness Evaluation Repository

## ğŸ“Œ Overview

This repository contains code and results for the **Politeness
Evaluation Study**.\
The goal is to evaluate language models on their ability to produce
polite responses.\
We implement and compare **three complementary evaluation approaches**:

1.  **GPT Continuous Scoring** (`gpt_eval/`)
    -   Judges politeness on a continuous scale **\[-1, 1\]**.\
    -   Example: "Please fix this error." â†’ 0.0, "I would appreciate it
        if..." â†’ 0.8.
2.  **Frequency-based Scoring** (`frequency/`)
    -   Counts the occurrence of **polite markers** (e.g., *please,
        thank you, sorry, appreciate*).\
    -   Outputs word- or sentence-level frequency metrics.
3.  **Polite-Guard Evaluation** (`polite_guard_eval/`)
    -   Uses **HuggingFace's Polite-Guard model** for automatic
        politeness classification.\
    -   Supports **layer-wise analysis** (probing intermediate
        representations).

------------------------------------------------------------------------

## ğŸ“‚ Repository Structure

    repo_root/
    â”‚
    â”œâ”€â”€ README.md                 # This file (overview)
    â”œâ”€â”€ requirements.txt          # Dependencies
    â”‚
    â”œâ”€â”€ evaluation/
    â”‚   â”œâ”€â”€ gpt_eval/             # GPT continuous scoring (-1..1)
    â”‚   â”‚   â”œâ”€â”€ politeness_score_batch_chunks_resume_logged.py
    â”‚   â”‚   â”œâ”€â”€ scores-*.csv
    â”‚   â”‚   â”œâ”€â”€ Average_score_per_model-*.csv
    â”‚   â”‚   â””â”€â”€ README.md
    â”‚   â”‚
    â”‚   â”œâ”€â”€ frequency/            # Polite-word frequency scoring
    â”‚   â”‚   â”œâ”€â”€ sentence-score.py
    â”‚   â”‚   â”œâ”€â”€ word-score.py
    â”‚   â”‚   â”œâ”€â”€ gpt_politeword_freq.py
    â”‚   â”‚   â”œâ”€â”€ scores-*-sentence.csv / word.csv
    â”‚   â”‚   â”œâ”€â”€ Average_score_sentence-*.csv / word-*.csv
    â”‚   â”‚   â””â”€â”€ README.md
    â”‚   â”‚
    â”‚   â”œâ”€â”€ polite_guard_eval/    # HuggingFace Polite-Guard evaluation
    â”‚   â”‚   â”œâ”€â”€ run_eval.py
    â”‚   â”‚   â”œâ”€â”€ scores-*.csv
    â”‚   â”‚   â””â”€â”€ README.md
    â”‚   â”‚
    â”‚   â””â”€â”€ README.md             # (Optional) summary for all evaluation methods
    â”‚
    â””â”€â”€ data/                     # (Optional) Instructions on dataset retrieval

------------------------------------------------------------------------

## â–¶ï¸ Quick Start

### 1. Install dependencies

``` bash
pip install -r requirements.txt
```

### 2. Prepare dataset

-   Input: JSON files containing responses.\
-   Each record must follow:

``` json
{
  "model": "model_name",
  "query_id": "123",
  "answer": "Your response text here"
}
```

-   Dataset is **not included**. Please follow project/course
    instructions to obtain it.

### 3. Run evaluations

#### GPT continuous scoring

``` bash
python evaluation/gpt_eval/politeness_score_batch_chunks_resume_logged.py     --input_dir data/     --out evaluation/gpt_eval/scores-1B.csv     --model gpt-4o-mini     --chunk_size 100     --rpm 200
```

#### Frequency-based scoring

``` bash
python evaluation/frequency/gpt_politeword_freq.py     --input_dir data/     --out evaluation/frequency/scores-1B-sentence.csv     --model gpt-4o-mini
```

#### Polite-Guard evaluation

``` bash
python evaluation/polite_guard_eval/run_eval.py     --input_dir data/     --out evaluation/polite_guard_eval/scores-1B.csv
```

------------------------------------------------------------------------

## ğŸ“‘ Outputs

-   `scores-*.csv` â†’ raw per-response politeness scores\
-   `Average_score_*.csv` â†’ aggregated mean politeness per model\
-   `raw_outputs/` â†’ raw completions from GPT-based evaluators

------------------------------------------------------------------------

## ğŸ† Alignment with Expectations & Grading

-   **Complete materials**: Scripts, results, documentation provided.\
-   **Well-documented**: Top-level overview + per-folder READMEs.\
-   **Reproducible**: Instructions to install/run, resumable scripts.\
-   **Functional**: All methods return results in a standardized CSV
    format.\
-   **Adequate**: Implements multiple methods to triangulate politeness
    measurement.\
-   **Dataset handling**: Instructions provided; dataset not included.\
-   **Extra**: Cross-method comparison and multiple evaluation metrics.

------------------------------------------------------------------------

## âœ¨ Notes

-   Each evaluation method is modular and can be extended with new
    datasets or models.\
-   Results are directly comparable since they follow a **standard CSV
    format**.\
-   The repo is structured for **clarity and grading transparency**.
